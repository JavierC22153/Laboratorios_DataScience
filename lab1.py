# -*- coding: utf-8 -*-
"""Lab1_DataScience(Avance).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tcz5lnpmdRWwWTvqNuQznZHT3j0l60bP
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import shapiro
from statsmodels.tsa.seasonal import seasonal_decompose
import numpy as np
from pandas.plotting import autocorrelation_plot
import warnings
warnings.filterwarnings('ignore')

"""# Analisis Exploratorio"""

sns.set(style="whitegrid")

# Variables continuas
variables = ['gasolina_regular', 'gasolina_superior', 'diesel', 'gas_licuado']

# -------------------------------
# Función para detectar encabezado correcto
# -------------------------------
def detectar_header(df):
    for i in range(10):
        row = df.iloc[i].astype(str).str.lower()
        if any(word in row.to_string() for word in ['regular', 'super', 'súper', 'diesel', 'gas']):
            return i
    return 0

def cargar_con_encabezado(path):
    preview = pd.read_excel(path, header=None, nrows=15)
    header_row = detectar_header(preview)
    df = pd.read_excel(path, header=header_row)
    df.columns = df.columns.str.lower().str.strip()
    return df

# -------------------------------
# Cargar archivos
# -------------------------------
files = {
    "consumo_2024": "CONSUMO-HIDROCARBUROS-2024-12.xlsx",
    "ventas_2025": "VENTAS-HIDROCARBUROS-2025-05.xlsx",
    "import_2024": "IMPORTACION-HIDROCARBUROS-VOLUMEN-2024-12.xlsx",
    "import_2025": "IMPORTACION-HIDROCARBUROS-VOLUMEN-2025-05.xlsx",
    "precios": "Precios-Promedio-Nacionales-Diarios-2025-1.xlsx"
}

dfs = {k: cargar_con_encabezado(path) for k, path in files.items()}

# -------------------------------
# Función para preparar DataFrame de hidrocarburos
# -------------------------------
def preparar_df(df, fuente):
    df = df.copy()
    df['fecha'] = pd.to_datetime(df['fecha'], errors='coerce')
    df = df.dropna(subset=['fecha'])

    for col in ['diesel bajo azufre', 'diesel ultra bajo azufre', 'diesel alto azufre']:
        if col not in df.columns:
            df[col] = 0

    df_filtrado = pd.DataFrame({
        'fecha': df['fecha'],
        'gasolina_regular': df.get('gasolina regular', 0),
        'gasolina_superior': df.get('gasolina superior', 0),
        'gas_licuado': df.get('gas licuado de petróleo', 0),
        'diesel': df['diesel bajo azufre'] + df['diesel ultra bajo azufre'] + df['diesel alto azufre'],
        'fuente': fuente
    })
    return df_filtrado

# -------------------------------
# Preparar conjuntos combinados
# -------------------------------

# Consumo total
consumo_total = pd.concat([
    preparar_df(dfs["consumo_2024"], "consumo"),
    preparar_df(dfs["ventas_2025"], "ventas")
], ignore_index=True)

# Importación total
importacion_total = pd.concat([
    preparar_df(dfs["import_2024"], "importacion"),
    preparar_df(dfs["import_2025"], "importacion")
], ignore_index=True)

# Datos de precios
precios = dfs["precios"].copy()
precios.columns = precios.columns.str.lower().str.strip()
precios = precios.rename(columns={
    "superior": "gasolina_superior",
    "regular": "gasolina_regular",
    "diesel": "diesel",
    "glp cilindro 25lbs.": "gas_licuado"
})
precios['fecha'] = pd.to_datetime(precios['fecha'], errors='coerce')
precios = precios.dropna(subset=['fecha'])

# -------------------------------
# Mostrar los resultados
# -------------------------------
print("🔥 Datos de consumo_total:")
print(consumo_total.head())

print("\n🚢 Datos de importacion_total:")
print(importacion_total.head())

print("\n💰 Datos de precios diarios:")
print(precios[['fecha', 'gasolina_regular', 'gasolina_superior', 'diesel', 'gas_licuado']].head())

print("="*60)
print("1. ANÁLISIS DE NORMALIDAD PARA TODAS LAS VARIABLES")
print("="*60)

datasets = {
    'Importación': importacion_total,
    'Consumo/Ventas': consumo_total
}

for dataset_name, dataset in datasets.items():
    print(f"\n📊 DATASET: {dataset_name}")
    print("-" * 40)

    for var in variables:
        if var in dataset.columns:
            data = dataset[var].dropna()
            if len(data) > 0:
                # Estadísticos descriptivos
                print(f"\n🔍 Variable: {var}")
                print(f"   Media: {data.mean():.2f}")
                print(f"   Mediana: {data.median():.2f}")
                print(f"   Desv. Estándar: {data.std():.2f}")
                print(f"   Asimetría: {data.skew():.3f}")
                print(f"   Curtosis: {data.kurtosis():.3f}")

                # Prueba de normalidad
                muestra = data.sample(min(5000, len(data)), random_state=42)
                stat, p = shapiro(muestra)
                print(f"   Shapiro-Wilk: Estadístico={stat:.3f}, p-valor={p:.3f}")
                normalidad = "SÍ" if p > 0.05 else "NO"
                print(f"   ¿Es normal?: {normalidad}")

print("\n" + "="*60)
print("2. ANÁLISIS TEMPORAL DETALLADO")
print("="*60)

for dataset_name, dataset in datasets.items():
    dataset['año'] = dataset['fecha'].dt.year
    dataset['mes'] = dataset['fecha'].dt.month

print("\n📈 IMPORTACIONES POR AÑO Y TIPO DE COMBUSTIBLE")
print("-" * 50)
importaciones_año = importacion_total.groupby('año')[variables].sum()
print(importaciones_año)

fig, axes = plt.subplots(2, 2, figsize=(15, 10))
axes = axes.ravel()

for i, var in enumerate(variables):
    importaciones_año[var].plot(kind='bar', ax=axes[i], color=plt.cm.Set3(i))
    axes[i].set_title(f'Importación anual: {var}')
    axes[i].set_xlabel('Año')
    axes[i].set_ylabel('Volumen')
    axes[i].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.suptitle('IMPORTACIONES ANUALES POR COMBUSTIBLE', y=1.02, fontsize=16)
plt.savefig("imagenes/ImportacionesAnualesXCombustible.png")

print("\n📅 ANÁLISIS ESTACIONAL - MESES CON MAYOR IMPORTACIÓN")
print("-" * 55)
meses_nombres = ['Ene', 'Feb', 'Mar', 'Abr', 'May', 'Jun',
                'Jul', 'Ago', 'Sep', 'Oct', 'Nov', 'Dic']

mes_importaciones = importacion_total.groupby('mes')[variables].sum()
mes_importaciones.index = meses_nombres

print("Ranking de meses por importación total:")
total_por_mes = mes_importaciones.sum(axis=1).sort_values(ascending=False)
for i, (mes, total) in enumerate(total_por_mes.items(), 1):
    print(f"{i:2d}. {mes}: {total:,.0f}")

plt.figure(figsize=(12, 6))
sns.heatmap(mes_importaciones.T, annot=True, fmt='.0f', cmap='YlOrRd')
plt.title('Mapa de Calor: Importaciones por Mes y Combustible')
plt.xlabel('Mes')
plt.ylabel('Tipo de Combustible')
plt.savefig("imagenes/MapaDeCalorImportacionesXMesyCombustible.png")

print("\n🔥 PICOS ANUALES POR TIPO DE COMBUSTIBLE")
print("-" * 45)
for var in variables:
    año_max = importaciones_año[var].idxmax()
    valor_max = importaciones_año[var].max()
    print(f"{var:20} - Pico en {año_max}: {valor_max:,.0f}")

print("\n" + "="*60)
print("3. ANÁLISIS COMPARATIVO CONSUMO VS IMPORTACIÓN")
print("="*60)

# Crear series mensuales para comparación
fig, axes = plt.subplots(2, 2, figsize=(16, 10))
axes = axes.ravel()

for i, var in enumerate(variables):
    # Series mensuales
    imp_mensual = importacion_total.set_index('fecha')[var].resample('ME').sum()
    con_mensual = consumo_total.set_index('fecha')[var].resample('ME').sum()

    # Gráfico
    axes[i].plot(imp_mensual.index, imp_mensual.values,
                label='Importación', linewidth=2, alpha=0.8)
    axes[i].plot(con_mensual.index, con_mensual.values,
                label='Consumo/Ventas', linewidth=2, alpha=0.8)
    axes[i].set_title(f'{var.replace("_", " ").title()}')
    axes[i].legend()
    axes[i].grid(True, alpha=0.3)

    # Correlación
    common_data = pd.concat([imp_mensual, con_mensual], axis=1, join='inner')
    if len(common_data) > 1:
        corr = common_data.iloc[:, 0].corr(common_data.iloc[:, 1])
        axes[i].text(0.05, 0.95, f'Corr: {corr:.3f}',
                    transform=axes[i].transAxes,
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7))

plt.tight_layout()
plt.suptitle('COMPARACIÓN TEMPORAL: IMPORTACIÓN VS CONSUMO', y=1.02, fontsize=16)
plt.savefig("imagenes/ImportacionVSConsumo.png")

# Variables continuas
# Removed the definition of variables from here as it was moved to the previous cell.

# Histograma + KDE para ver distribución
for var in variables:
    plt.figure(figsize=(8,4))
    sns.histplot(importacion_total[var], kde=True, bins=30)
    plt.title(f'Distribución de importación: {var}')
    plt.savefig("imagenes/DistribucionDeImportacion.png")

    # Prueba de normalidad Shapiro-Wilk (muestra aleatoria de 5000 para evitar error)
    muestra = importacion_total[var].dropna().sample(min(5000, len(importacion_total)), random_state=42)
    stat, p = shapiro(muestra)
    print(f'Prueba Shapiro para {var}: Estadístico={stat:.3f}, p-valor={p:.3f}')
    if p > 0.05:
        print(f"-> La variable {var} parece seguir una distribución normal (no se rechaza H0)\n")
    else:
        print(f"-> La variable {var} NO sigue una distribución normal (rechazamos H0)\n")

# Agregar por mes y combustible
importacion_total['mes'] = importacion_total['fecha'].dt.month
mes_importaciones = importacion_total.groupby('mes')[variables].sum()

plt.figure(figsize=(12,6))
mes_importaciones.plot(kind='bar')
plt.title("Importaciones por mes y tipo de combustible")
plt.xlabel("Mes")
plt.ylabel("Volumen importado")
plt.xticks(rotation=0)
plt.savefig("imagenes/ImportacionesXMEsyTipo.png")

"""# Análisis preliminar de 2 series"""

# Bloque para importar librerias para hacer el analisis de las series
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.arima.model import ARIMA
import matplotlib.dates as mdates
from prophet import Prophet
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error
import numpy as np
from statsmodels.tsa.holtwinters import ExponentialSmoothing
import itertools

print("="*30)
print("ANÁLISIS DE SERIES TEMPORALES")
print("="*30)

# 2.1 CREAR LAS 3 SERIES SELECCIONADAS
print("\n CREACIÓN DE LAS 3 SERIES SELECCIONADAS")
print("-" * 45)

# Serie 1 - Importación mensual de diesel (volumen)
serie_importacion_diesel = importacion_total.set_index('fecha')['diesel'].resample('ME').sum()
serie_importacion_diesel = serie_importacion_diesel.dropna()

# Serie 2 - Consumo mensual de gasolina superior
serie_consumo_gasolina = consumo_total.set_index('fecha')['gasolina_superior'].resample('ME').sum()
serie_consumo_gasolina = serie_consumo_gasolina.dropna()

# Serie 3 - Precios diarios de gasolina regular (necesitamos convertir a mensual)
precios_mensuales = precios.set_index('fecha')['gasolina_regular'].resample('ME').mean()
precios_mensuales = pd.to_numeric(precios_mensuales, errors='coerce').dropna() # Convert to numeric and drop NaNs

# Diccionario con las 3 series
series_analizar = {
    'Importación Diesel': serie_importacion_diesel,
    'Consumo Gasolina Superior': serie_consumo_gasolina,
    'Precios Gasolina Regular': precios_mensuales
}

print("\nESPECIFICACIONES DE LAS SERIES")
print("-" * 35)

for nombre, serie in series_analizar.items():
    print(f"\n {nombre}:")
    print(f"   Inicio: {serie.index.min().strftime('%Y-%m-%d')}")
    print(f"   Fin: {serie.index.max().strftime('%Y-%m-%d')}")
    print(f"   Frecuencia: Mensual")
    print(f"   Observaciones: {len(serie)}")
    print(f"   Rango: {serie.min():.2f} - {serie.max():.2f}")

print("\n VISUALIZACIÓN DE LAS SERIES")
print("-" * 32)

fig, axes = plt.subplots(3, 1, figsize=(15, 12))

for i, (nombre, serie) in enumerate(series_analizar.items()):
    axes[i].plot(serie.index, serie.values, linewidth=2, color=plt.cm.Set1(i))
    axes[i].set_title(f'{nombre}', fontsize=14, fontweight='bold')
    axes[i].grid(True, alpha=0.3)
    axes[i].set_ylabel('Valor')

    from scipy import stats
    x_numeric = pd.to_numeric(serie.index)
    slope, intercept, r_value, p_value, std_err = stats.linregress(x_numeric, serie.values)
    line = slope * x_numeric + intercept
    axes[i].plot(serie.index, line, '--', color='red', alpha=0.7,
                label=f'Tendencia (R²={r_value**2:.3f})')
    axes[i].legend()

    axes[i].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
    axes[i].xaxis.set_major_locator(mdates.YearLocator())

plt.tight_layout()
plt.suptitle('SERIES TEMPORALES SELECCIONADAS', y=1.02, fontsize=16)
plt.savefig("imagenes/SeriesTemporales.png")

print("\n DESCOMPOSICIÓN DE LAS SERIES")
print("-" * 32)

for nombre, serie in series_analizar.items():
    print(f"\n Descomponiendo: {nombre}")

    if len(serie) >= 24:
        try:
            decomposition = seasonal_decompose(serie, model='additive', period=12)

            fig, axes = plt.subplots(4, 1, figsize=(15, 10))
            decomposition.observed.plot(ax=axes[0], title='Serie Original')
            decomposition.trend.plot(ax=axes[1], title='Tendencia')
            decomposition.seasonal.plot(ax=axes[2], title='Estacional')
            decomposition.resid.plot(ax=axes[3], title='Residuos')

            plt.suptitle(f'Descomposición: {nombre}', fontsize=14)
            plt.tight_layout()
            plt.savefig(f"imagenes/DescomposicionDeSeries{nombre}.png")

            # Análisis de componentes
            print(f"    Tendencia: {'Creciente' if decomposition.trend.dropna().iloc[-1] > decomposition.trend.dropna().iloc[0] else 'Decreciente'}")
            print(f"    Estacionalidad: Presente (variación: {decomposition.seasonal.std():.2f})")
            print(f"    Residuos: Varianza = {decomposition.resid.var():.2f}")

        except Exception as e:
            print(f"Error en descomposición: {str(e)}")
    else:
        print(f"Datos insuficientes para descomposición ({len(serie)} observaciones)")

# ANÁLISIS DE ESTACIONARIEDAD

print("ANÁLISIS DE ESTACIONARIEDAD Y TRANSFORMACIONES")

def test_estacionariedad(serie, nombre, alpha=0.05):
    """
    Realiza el test de Dickey-Fuller Aumentado para estacionariedad
    """
    print(f"\n ANÁLISIS DE ESTACIONARIEDAD: {nombre}")

    # Test de Dickey-Fuller Aumentado
    result = adfuller(serie.dropna())

    print(f"   Resultados del Test ADF:")
    print(f"   Estadístico ADF: {result[0]:.4f}")
    print(f"   p-valor: {result[1]:.4f}")
    print(f"   Valores críticos:")
    for key, value in result[4].items():
        print(f"      {key}: {value:.4f}")

    # Interpretación
    if result[1] <= alpha:
        print(f"La serie ES estacionaria (p-valor < {alpha})")
        estacionaria = True
    else:
        print(f"La serie NO ES estacionaria (p-valor >= {alpha})")
        estacionaria = False

    return estacionaria, result

def analizar_autocorrelacion(serie, nombre, lags=20):
    """
    Analiza autocorrelación y autocorrelación parcial
    """
    print(f"\n📈 ANÁLISIS DE AUTOCORRELACIÓN: {nombre}")

    fig, axes = plt.subplots(2, 1, figsize=(12, 8))

    # Adjust lags based on series length
    nobs = len(serie.dropna())
    lags_to_use = min(lags, nobs // 2 - 1) # Common heuristic for max lags

    # ACF
    plot_acf(serie.dropna(), lags=lags_to_use, ax=axes[0])
    axes[0].set_title(f'Función de Autocorrelación (ACF) - {nombre}')

    # PACF
    plot_pacf(serie.dropna(), lags=lags_to_use, ax=axes[1])
    axes[1].set_title(f'Función de Autocorrelación Parcial (PACF) - {nombre}')

    plt.tight_layout()
    plt.savefig(f"imagenes/AutocorrelacionPACF {nombre}.png")

    if nobs > 1:
        acf_values = pd.Series(serie.dropna()).autocorr(lag=1)
        print(f"Autocorrelación lag-1: {acf_values:.3f}")

        if abs(acf_values) > 0.7:
            print("Interpretación: Fuerte autocorrelación, sugiere no estacionariedad")
        elif abs(acf_values) > 0.3:
            print("Interpretación: Autocorrelación moderada")
        else:
            print("Interpretación: Autocorrelación débil")
    else:
        print("Interpretación: No hay suficientes datos para calcular la autocorrelación lag-1")


def diferencias_serie(serie, nombre, max_diff=3):
    """
    Aplica diferenciación hasta lograr estacionariedad
    """
    print(f"\nDIFERENCIACIÓN: {nombre}")

    serie_original = serie.copy()
    d = 0

    for i in range(max_diff):
        estacionaria, _ = test_estacionariedad(serie, f"{nombre} (d={d})", alpha=0.05)

        if estacionaria:
            print(f"Serie estacionaria con d={d}")
            break
        else:
            print(f"Aplicando diferencia {d+1}...")
            serie = serie.diff().dropna()
            d += 1

    if d > 0:
        print(f"\nSerie final después de {d} diferencia(s):")
        print(f"    Media: {serie.mean():.4f}")
        print(f"    Varianza: {serie.var():.4f}")

        # Graficar serie diferenciada
        plt.figure(figsize=(12, 6))
        plt.subplot(2, 1, 1)
        plt.plot(serie_original, label='Serie Original')
        plt.title(f'{nombre} - Serie Original')
        plt.legend()
        plt.grid(True, alpha=0.3)

        plt.subplot(2, 1, 2)
        plt.plot(serie, label=f'Diferenciada (d={d})', color='orange')
        plt.title(f'{nombre} - Serie Diferenciada')
        plt.legend()
        plt.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(f"imagenes/SerieFinal{nombre}.png")

    return serie, d

resultados_estacionariedad = {}

for nombre, serie in series_analizar.items():
    print("\n" + "="*60)
    print(f"ANÁLISIS COMPLETO: {nombre}")
    print("="*60)

    # Test inicial de estacionariedad
    estacionaria_inicial, resultado_adf = test_estacionariedad(serie, nombre)

    # Análisis de autocorrelación
    analizar_autocorrelacion(serie, nombre)

    # Diferenciación si es necesaria
    if not estacionaria_inicial:
        serie_diferenciada, d_necesario = diferencias_serie(serie, nombre)

        if d_necesario > 0:
            analizar_autocorrelacion(serie_diferenciada, f"{nombre} (diferenciada)")
    else:
        serie_diferenciada = serie
        d_necesario = 0

    resultados_estacionariedad[nombre] = {
        'serie_original': serie,
        'serie_estacionaria': serie_diferenciada,
        'es_estacionaria_inicial': estacionaria_inicial,
        'd_necesario': d_necesario,
        'resultado_adf': resultado_adf
    }


print("Resumen del analisis")


for nombre, resultados in resultados_estacionariedad.items():
    print(f"\n {nombre}:")
    print(f"   Estacionaria inicialmente: {'SÍ' if resultados['es_estacionaria_inicial'] else 'NO'}")
    print(f"   Diferencias necesarias (d): {resultados['d_necesario']}")
    print(f"   ADF Estadístico: {resultados['resultado_adf'][0]:.4f}")
    print(f"   ADF p-valor: {resultados['resultado_adf'][1]:.4f}")

    if resultados['d_necesario'] == 0:
        print(f"    Recomendación: Usar modelo ARMA")
    else:
        print(f"    Recomendación: Usar modelo ARIMA con d={resultados['d_necesario']}")

print("="*80)
print("INSTRUCCIÓN 4: MODELOS DE PREDICCIÓN")
print("="*80)

from pmdarima.arima import auto_arima
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from prophet import Prophet
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler


def crear_conjuntos_entrenamiento_prueba(serie, test_years=3, min_train_obs=24):
    """
    Divide la serie en train/test:
      - test_years: años aproximados para test
      - min_train_obs: mínimo de meses para train
    """
    # asegúrate de que la serie tenga freq mensual
    if serie.index.freq is None:
        serie = serie.asfreq('MS')
    
    # número de meses de test deseados
    meses_test = test_years * 12

    # si la serie es demasiado corta ajusta meses_test
    total = len(serie)
    if total - meses_test < min_train_obs:
        # deja al menos 'min_train_obs' para train,
        # o, si la serie es muy corta, asigna 20% a test
        meses_test = max(total - min_train_obs,
                         int(total * 0.2))
        meses_test = max(meses_test, 0)

    fecha_corte = serie.index[-meses_test] if meses_test > 0 else serie.index[0]

    train = serie[serie.index <= fecha_corte]
    test = serie[serie.index > fecha_corte]

    return train, test, fecha_corte


def evaluar_modelo(y_true, y_pred, nombre_modelo):
    """
    Evalúa un modelo usando múltiples métricas
    """
    # Ensure y_true and y_pred are not empty
    if len(y_true) == 0 or len(y_pred) == 0:
        print(f"\n Métricas para {nombre_modelo}:")
        print("   No hay datos para evaluar.")
        return {'MAE': np.nan, 'RMSE': np.nan, 'MAPE': np.nan}

    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    # Avoid division by zero if y_true contains zero values
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100 if np.all(y_true != 0) else np.nan


    print(f"\n Métricas para {nombre_modelo}:")
    print(f"   MAE: {mae:.4f}")
    print(f"   RMSE: {rmse:.4f}")
    print(f"   MAPE: {mape:.2f}%" if not np.isnan(mape) else "   MAPE: N/A")

    return {'MAE': mae, 'RMSE': rmse, 'MAPE': mape}

def modelo_arima_grid_search(train_data, max_p=5, max_q=5, max_d=2):
    """
    Búsqueda en grilla para encontrar el mejor modelo ARIMA
    """
    best_aic = float('inf')
    best_params = None
    best_model = None

    print("🔍 Búsqueda de mejores parámetros ARIMA...")

    # Ensure enough data for ARIMA
    if len(train_data) < max_p + max_d + max_q + 1:
        print("   Datos insuficientes para grid search ARIMA con los parámetros especificados.")
        return None, None


    for p in range(max_p + 1):
        for d in range(max_d + 1):
            for q in range(max_q + 1):
                try:
                    model = ARIMA(train_data, order=(p, d, q))
                    fitted_model = model.fit()

                    if fitted_model.aic < best_aic:
                        best_aic = fitted_model.aic
                        best_params = (p, d, q)
                        best_model = fitted_model

                except:
                    continue

    if best_params:
        print(f"   Mejores parámetros: ARIMA{best_params}")
        print(f"   AIC: {best_aic:.2f}")
    else:
        print("   No se encontraron parámetros válidos para ARIMA.")


    return best_model, best_params

def modelo_prophet(train_data):
    """
    Modelo Prophet de Facebook
    """
    # Ensure enough data for Prophet
    if len(train_data) < 2: # Prophet requires at least 2 data points
        print("   Datos insuficientes para Prophet.")
        return None

    # Preparar datos para Prophet
    df_prophet = pd.DataFrame({
        'ds': train_data.index,
        'y': train_data.values
    })

    # Crear y entrenar modelo
    model = Prophet(
        yearly_seasonality=True,
        weekly_seasonality=False,
        daily_seasonality=False,
        seasonality_mode='additive'
    )

    model.fit(df_prophet)
    return model

def modelo_holt_winters(train_data):
    """
    Modelo Holt-Winters
    """
    # Ensure enough data for Holt-Winters (at least one full season + trend)
    if len(train_data) < 25: # Trend ('add') + Seasonal ('add' or 'mul') with period=12
        print("   Datos insuficientes para Holt-Winters con estacionalidad (periodo 12).")
        try:
            # Try simpler model if not enough data for seasonal
            model = ExponentialSmoothing(
                train_data,
                trend='add'
            ).fit()
            print("   Entrenando modelo Holt-Winters sin estacionalidad.")
            return model
        except:
             print("   Datos insuficientes para Holt-Winters sin estacionalidad.")
             return None


    try:
        model = ExponentialSmoothing(
            train_data,
            trend='add',
            seasonal='add',
            seasonal_periods=12
        ).fit()
        return model
    except:
        # Si falla el modelo aditivo, probar multiplicativo
        try:
            model = ExponentialSmoothing(
                train_data,
                trend='add',
                seasonal='mul',
                seasonal_periods=12
            ).fit()
            return model
        except:
            # If both fail, return None or raise error
            print("   Error al entrenar modelo Holt-Winters con estacionalidad.")
            return None


def modelo_red_neuronal(train_data, n_lags=12):
    """
    Modelo de Red Neuronal para series temporales
    """
    # Ensure enough data for Neural Network (at least n_lags + 1 for a single sample)
    if len(train_data) < n_lags + 1:
        print(f"   Datos insuficientes para Red Neuronal (necesita al menos {n_lags + 1} observaciones).")
        return None, None, None

    # Crear características con lags
    X, y = [], []

    for i in range(n_lags, len(train_data)):
        X.append(train_data.iloc[i-n_lags:i].values)
        y.append(train_data.iloc[i])

    X, y = np.array(X), np.array(y)

    # Ensure there are samples after creating lags
    if len(X) == 0:
         print("   No se pudieron crear suficientes muestras con lags para la Red Neuronal.")
         return None, None, None

    # Escalar datos
    scaler_X = StandardScaler()
    scaler_y = StandardScaler()

    X_scaled = scaler_X.fit_transform(X)
    y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()

    # Crear y entrenar modelo
    model = MLPRegressor(
        hidden_layer_sizes=(100, 50),
        max_iter=1000,
        random_state=42,
        alpha=0.01
    )

    model.fit(X_scaled, y_scaled)

    return model, scaler_X, scaler_y

# Diccionario para almacenar todos los resultados
resultados_modelos = {}

for nombre_serie, serie in series_analizar.items():
    print(f"\n{'='*60}")
    print(f"ANALIZANDO: {nombre_serie}")
    print(f"{'='*60}")

    # Dividir en entrenamiento y prueba
    train, test, fecha_corte = crear_conjuntos_entrenamiento_prueba(serie)

    print(f"\n📅 División de datos:")
    if not train.empty:
        print(f"   Entrenamiento: {train.index.min().strftime('%Y-%m')} a {train.index.max().strftime('%Y-%m')} ({len(train)} obs)")
    else:
        print("   Entrenamiento: No hay suficientes datos para el conjunto de entrenamiento.")

    if not test.empty:
        print(f"   Prueba: {test.index.min().strftime('%Y-%m')} a {test.index.max().strftime('%Y-%m')} ({len(test)} obs)")
    else:
        print("   Prueba: No hay suficientes datos para el conjunto de prueba.")


    if len(test) == 0 or len(train) == 0:
        print("⚠️  No hay suficientes datos para realizar el modelado y la predicción.")
        resultados_modelos[nombre_serie] = {
            'train': train,
            'test': test,
            'modelos': {},
            'predicciones': {},
            'fecha_corte': fecha_corte
        }
        continue


    modelos_serie = {}
    predicciones = {}

    # 1. MODELOS ARIMA
    print(f"\n🎯 MODELOS ARIMA")
    print("-" * 25)

    # Auto ARIMA
    try:
        if len(train) > 1: # Auto ARIMA needs at least 2 data points
            auto_model = auto_arima(train, seasonal=True, m=12,
                                   suppress_warnings=True, stepwise=True)
            if auto_model:
                 auto_pred = auto_model.predict(n_periods=len(test))
                 modelos_serie['Auto ARIMA'] = evaluar_modelo(test.values, auto_pred, 'Auto ARIMA')
                 predicciones['Auto ARIMA'] = auto_pred
                 print(f"   Modelo seleccionado: ARIMA{auto_model.order}")
                 if hasattr(auto_model, 'seasonal_order'):
                     print(f"   Estacional: {auto_model.seasonal_order}")
            else:
                 print("   Auto ARIMA no pudo encontrar un modelo válido.")


    except Exception as e:
        print(f"Error en Auto ARIMA: {e}")

    # Grid Search ARIMA
    try:
        best_arima, best_params = modelo_arima_grid_search(train)
        if best_arima is not None:
            arima_pred = best_arima.forecast(steps=len(test))
            modelos_serie['Best ARIMA'] = evaluar_modelo(test.values, arima_pred, 'Best ARIMA')
            predicciones['Best ARIMA'] = arima_pred
        else:
            print("   Grid Search ARIMA no pudo encontrar un modelo válido.")
    except Exception as e:
        print(f"Error en Grid Search ARIMA: {e}")

    # 2. PROPHET
    print(f"\n🔮 MODELO PROPHET")
    print("-" * 20)

    try:
        prophet_model = modelo_prophet(train)
        if prophet_model:
            # Crear fechas futuras
            future = prophet_model.make_future_dataframe(periods=len(test), freq='MS')
            forecast = prophet_model.predict(future)

            prophet_pred = forecast.tail(len(test))['yhat'].values
            modelos_serie['Prophet'] = evaluar_modelo(test.values, prophet_pred, 'Prophet')
            predicciones['Prophet'] = prophet_pred
        else:
            print("   Prophet no pudo ser entrenado.")

    except Exception as e:
        print(f"Error en Prophet: {e}")

    # 3. HOLT-WINTERS
    print(f"\n❄️  MODELO HOLT-WINTERS")
    print("-" * 25)

    try:
        hw_model = modelo_holt_winters(train)
        if hw_model:
            hw_pred = hw_model.forecast(steps=len(test))

            modelos_serie['Holt-Winters'] = evaluar_modelo(test.values, hw_pred, 'Holt-Winters')
            predicciones['Holt-Winters'] = hw_pred
        else:
            print("   Holt-Winters no pudo ser entrenado.")

    except Exception as e:
        print(f"Error en Holt-Winters: {e}")


    # 4. RED NEURONAL
    print(f"\n🧠 MODELO RED NEURONAL")
    print("-" * 25)

    try:
        nn_model, scaler_X, scaler_y = modelo_red_neuronal(train)
        if nn_model:
            # Generar predicciones
            nn_predictions = []
            last_sequence = train.tail(12).values

            for _ in range(len(test)):
                # Predecir siguiente valor
                X_pred = scaler_X.transform(last_sequence.reshape(1, -1))
                y_pred_scaled = nn_model.predict(X_pred)
                y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1))[0, 0]

                nn_predictions.append(y_pred)

                # Actualizar secuencia (usar predicción)
                last_sequence = np.roll(last_sequence, -1)
                last_sequence[-1] = y_pred

            modelos_serie['Red Neuronal'] = evaluar_modelo(test.values, nn_predictions, 'Red Neuronal')
            predicciones['Red Neuronal'] = nn_predictions
        else:
            print("   Red Neuronal no pudo ser entrenada.")


    except Exception as e:
        print(f"Error en Red Neuronal: {e}")


    # Guardar resultados
    resultados_modelos[nombre_serie] = {
        'train': train,
        'test': test,
        'modelos': modelos_serie,
        'predicciones': predicciones,
        'fecha_corte': fecha_corte
    }

    # Visualizar resultados
    plt.figure(figsize=(15, 8))

    # Serie completa
    plt.plot(serie.index, serie.values, 'k-', label='Serie Original', linewidth=2)

    # Línea de separación
    if not train.empty and not test.empty and pd.notna(fecha_corte):
         plt.axvline(x=fecha_corte, color='red', linestyle='--', alpha=0.7, label='División Train/Test')


    # Predicciones
    colors = ['blue', 'green', 'orange', 'purple', 'brown']
    for i, (modelo, pred) in enumerate(predicciones.items()):
        if len(test) > 0 and modelo in modelos_serie and not np.isnan(modelos_serie[modelo]["MAPE"]): # Only plot if there are test data points and valid MAPE
            plt.plot(test.index, pred, color=colors[i % len(colors)],
                    label=f'{modelo} (MAE: {modelos_serie[modelo]["MAE"]:.1f}, RMSE: {modelos_serie[modelo]["RMSE"]:.1f}, MAPE: {modelos_serie[modelo]["MAPE"]:.1f}%)',
                    linewidth=2, alpha=0.8)
        elif len(test) > 0 and modelo in modelos_serie: # Plot if test data exists and MAPE is NaN
             plt.plot(test.index, pred, color=colors[i % len(colors)],
                    label=f'{modelo} (MAE: {modelos_serie[modelo]["MAE"]:.1f}, RMSE: {modelos_serie[modelo]["RMSE"]:.1f}, MAPE: N/A)',
                    linewidth=2, alpha=0.8)


    plt.title(f'Predicciones vs Realidad: {nombre_serie}', fontsize=14)
    plt.xlabel('Fecha')
    plt.ylabel('Valor')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(f"imagenes/PrediccionesVsRealidad{nombre_serie}.png")

from pmdarima.arima import auto_arima
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

print("PREDICCIONES PARA 2025")


predicciones_2025 = {}

for nombre_serie, serie in series_analizar.items():
    print(f"\n{'='*60}")
    print(f"PREDICCIONES 2025: {nombre_serie}")
    print(f"{'='*60}")

    if nombre_serie == 'Precios Gasolina Regular':
        train = serie  # 7 puntos
        n_pasados = len(train)
        n_restantes = 12 - n_pasados
        fechas_prueba = pd.date_range(
            start=train.index.max() + pd.DateOffset(months=1),
            periods=n_restantes,
            freq='MS'
        )
        try:
            modelo = auto_arima(
                train,
                seasonal=True,
                m=12,
                suppress_warnings=True,
                stepwise=True
            )
        except Exception as e:
            print("   ⚠️ auto_arima incluso sin estacionalidad falló:", e)
            continue
    
        pred = modelo.predict(n_periods=n_restantes)
        print("Predicción resto de 2025:")
        for f, v in zip(fechas_prueba, pred):
            print(f"  {f.strftime('%Y-%m')}: {v:.2f}")
        continue


    # Usar toda la serie para entrenar
    try:
        # Auto ARIMA para 2025
        modelo_2025 = auto_arima(serie, seasonal=True, m=12, suppress_warnings=True)

        # Crear fechas para 2025
        ultimo_mes = serie.index.max()
        fechas_2025 = pd.date_range(
            start=ultimo_mes + pd.DateOffset(months=1),
            periods=12,
            freq='MS'
        )

        # Generar predicción
        pred_2025 = modelo_2025.predict(n_periods=12)

        # Almacenar resultados
        predicciones_2025[nombre_serie] = {
            'fechas': fechas_2025,
            'valores': pred_2025,
            'modelo': str(modelo_2025.order)
        }

        print(f" Modelo utilizado: ARIMA{modelo_2025.order}")
        print(f" Período de predicción: {fechas_2025[0].strftime('%Y-%m')} a {fechas_2025[-1].strftime('%Y-%m')}")

        # Mostrar predicciones mensuales
        print(f"\n Predicciones mensuales para 2025:")
        for fecha, valor in zip(fechas_2025, pred_2025):
            print(f"   {fecha.strftime('%Y-%m')}: {valor:.2f}")

        # Estadísticas de la predicción
        print(f"\n Estadísticas de predicción 2025:")
        print(f"   Promedio: {np.mean(pred_2025):.2f}")
        print(f"   Mínimo: {np.min(pred_2025):.2f}")
        print(f"   Máximo: {np.max(pred_2025):.2f}")
        print(f"   Desviación estándar: {np.std(pred_2025):.2f}")

        # Comparar con datos históricos
        promedio_historico = serie.mean()
        cambio_porcentual = ((np.mean(pred_2025) - promedio_historico) / promedio_historico) * 100
        print(f"   Cambio vs promedio histórico: {cambio_porcentual:.1f}%")

        # Visualización
        plt.figure(figsize=(15, 8))

        # Datos históricos (últimos 3 años + predicción)
        fecha_inicio_viz = serie.index.max() - pd.DateOffset(years=3)
        serie_viz = serie[serie.index >= fecha_inicio_viz]

        plt.plot(serie_viz.index, serie_viz.values, 'b-', label='Datos Históricos', linewidth=2)
        plt.plot(fechas_2025, pred_2025, 'r-', label='Predicción 2025', linewidth=2, marker='o')

        # Banda de confianza (estimación simple)
        std_historica = serie.std()
        upper_bound = pred_2025 + 1.96 * std_historica
        lower_bound = pred_2025 - 1.96 * std_historica

        plt.fill_between(fechas_2025, lower_bound, upper_bound, alpha=0.3, color='red',
                        label='Banda de Confianza 95%')

        plt.title(f'Predicción 2025: {nombre_serie}', fontsize=14)
        plt.xlabel('Fecha')
        plt.ylabel('Valor')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(f"imagenes/Prediccion2025{nombre_serie}.png")

    except Exception as e:
        print(f"Error generando predicciones para {nombre_serie}: {e}")
